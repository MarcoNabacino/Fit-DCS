{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "`Fit-DCS` is a Python toolbox for modeling and analyzing diffuse correlation spectroscopy (DCS) data. It provides tools for both forward modeling\n",
    "and inverse fitting of DCS autocorrelation functions, allowing users to extract information about scatterer dynamics in turbid media.\n",
    "\n",
    "This Jupyter Notebook provides an introduction to the main features of the `Fit-DCS` toolbox, complete with code examples and exercises to familiarize\n",
    "users with its capabilities.\n",
    "\n",
    "# Forward modeling\n",
    "\n",
    "The simplest use of this toolbox is the modeling of the forward problem, that is, given the information on the scatterer dynamics\n",
    "(Brownian diffusion coefficient `db` and/or mean square velocity `v_ms` depending on the scatterer model) and the geometry (e.g., semi-infinite,\n",
    "slab, ...) calculate the corresponding autocorrelation functions `g1` and `g2`.\n",
    "\n",
    "The `forward` module contains the expressions for the normalized and unnormalized first-order autocorrelations `g1_norm` and `g1`\n",
    "obtained in the framework of the diffusion approximation to the correlation transport equation.\n",
    "It contains one submodule per geometry: for example, `forward.homogeneous_inf` contains the solution for the homogeneous\n",
    "semi-infinite geometry, `forward.homogeneous_slab` the ones for the homogeneous laterally infinite slab, and so on.\n",
    "\n",
    "In addition to a geometric model, to simulate the autocorrelation functions we also need a model for scatterer motion.\n",
    "These can be found in `forward.common`, and include Brownian motion, ballistic motion and hybrid motion with both components.\n",
    "\n",
    "## Semi-infinite geometry\n",
    "\n",
    "We'll start with a simple example of forward modeling in a semi-infinite geometry with Brownian motion.\n",
    "First of all, let's import the necessary modules and define the parameters of the simulation."
   ],
   "id": "df8cfbe5d6d922d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np                                          # Arrays\n",
    "import matplotlib.pyplot as plt                             # Plotting\n",
    "\n",
    "from fit_dcs.forward import common                          # Common functions (e.g., scatterer motion models)\n",
    "from fit_dcs.forward import homogeneous_semi_inf as hsi     # Semi-infinite geometry\n",
    "\n",
    "tau = np.logspace(-8, 0, 200)               # Delay times, s\n",
    "lambda0 = 785                               # Wavelength, nm\n",
    "mua = 0.1                                   # Absorption coefficient, 1/cm\n",
    "musp = 10                                   # Reduced scattering coefficient, 1/cm\n",
    "rho = 2.5                                   # Source-detector separation, cm\n",
    "n = 1.4                                     # Refractive index of the medium (external is 1)\n",
    "db = 1e-8                                   # Brownian diffusion coefficient, cm^2/s\n",
    "beta = 0.5                                  # Coherence of detected light"
   ],
   "id": "bc74f0dfd1061fd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that, in order to be flexible, the geometric models in `Fit-DCS` do not take directly the Brownian diffusion coefficient `db` as input,\n",
    "but rather the mean-square displacement (MSD) as a function of delay time `tau`.\n",
    "This allows the user to define custom scatterer motion models if needed.\n",
    "\n",
    "For simple Brownian motion, we can use the helper function `msd_brownian` in the `common` module to compute the MSD."
   ],
   "id": "7868baeb4e7546f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "msd = common.msd_brownian(tau, db)\n",
    "g1_norm = hsi.g1_norm(msd, mua, musp, rho, n, lambda0)"
   ],
   "id": "72ce433ada6df7ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now compute the second-order autocorrelation function `g2` using the Siegert relation and plot the resulting curves.",
   "id": "752767e6d01a6aff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "g2_norm = 1 + beta * g1_norm**2\n",
    "\n",
    "def plot_g1_g2(tau, g1, g2, labels=None):\n",
    "    \"\"\"\n",
    "    Helper function to plot g1 and g2. Supports multiple curves with labels.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "    axs[0].semilogx(tau, g1.T, label=labels)\n",
    "    axs[0].set_xlabel(r'$\\tau$ (s)')\n",
    "    axs[0].set_ylabel(r'$g_1$')\n",
    "    if labels is not None:\n",
    "        axs[0].legend()\n",
    "\n",
    "    axs[1].semilogx(tau, g2.T, label=labels)\n",
    "    axs[1].set_xlabel(r'$\\tau$ (s)')\n",
    "    axs[1].set_ylabel(r'$g_2$')\n",
    "    if labels is not None:\n",
    "        axs[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_g1_g2(tau, g1_norm, g2_norm)"
   ],
   "id": "55cda736e45772b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's now try different values of the Brownian diffusion coefficient `db` and see how they affect the autocorrelation functions.\n",
    "We will define `g1` as a 2D array whose axes are `(db, tau)`, and loop over different `db` values to compute `g1` for each.\n",
    "This 2D shape for `g1` and `g2` is what is expected by data analysis classes, where the first axis corresponds to\n",
    "different iterations and the second axis to delay times."
   ],
   "id": "7c590817744273f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "db_vals = [1e-9, 2e-9, 5e-9, 1e-8, 2e-8]\n",
    "g1_all = np.zeros((len(db_vals), len(tau)))\n",
    "for i in range(len(db_vals)):\n",
    "    msd = common.msd_brownian(tau, db_vals[i])\n",
    "    g1_all[i, :] = hsi.g1_norm(msd, mua, musp, rho, n, lambda0)\n",
    "g2_all = 1 + beta * g1_all**2\n",
    "\n",
    "plot_g1_g2(tau, g1_all, g2_all, labels=db_vals)  # We can pass the db values to be used in the legend"
   ],
   "id": "28d42b0e52250942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now try writing some code to simulate `g1` and `g2` for the same geometry but with ballistic scatterer motion.\n",
    "Use the function `common.msd_ballistic` to compute the mean-square displacement for ballistic motion.\n",
    "Hover over the function name to see its documentation, including the input parameters."
   ],
   "id": "962b60ce6c7fb677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "v_ms_vals = []  # Your code here... (mean square velocities, (cm/s)^2)\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "g1_all = np.zeros((len(v_ms_vals), len(tau)))\n",
    "g2_all = np.zeros((len(v_ms_vals), len(tau)))\n",
    "\n",
    "plot_g1_g2(tau, g1_all, g2_all, labels=v_ms_vals)"
   ],
   "id": "ea83cb7088577559",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, also try simulating `g1` and `g2` for a hybrid motion model with both Brownian and ballistic components.\n",
    "You can use the function `common.msd_hybrid` to compute the mean-square displacement for hybrid motion."
   ],
   "id": "62d3e2a65e06de13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "db_vals = []  # Your code here...\n",
    "v_ms = 1e-4\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "g1_all = np.zeros((len(db_vals), len(tau)))\n",
    "g2_all = np.zeros((len(db_vals), len(tau)))\n",
    "\n",
    "plot_g1_g2(tau, g1_all, g2_all, labels=db_vals)"
   ],
   "id": "9d87d41e837f0ae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Slab geometry\n",
    "\n",
    "The procedure for forward modeling in a slab geometry is similar to the semi-infinite case.\n",
    "We just need to import the appropriate module (`forward.homogeneous_inf_slab`) and define the additional\n",
    "parameters for the slab geometry.\n",
    "Since the theoretical model requires a sum over multiple image sources, we also need to define the number\n",
    "of image sources to include in the calculation (`m_max`).\n",
    "\n",
    "Note that two solutions are available for the slab geometry, one in reflectance (`g1_reflectance_norm`)\n",
    "and one in transmittance (`g1_transmittance_norm`).\n",
    "\n",
    "Let's see an example for the reflectance configuration."
   ],
   "id": "820313343cc667c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from fit_dcs.forward import homogeneous_inf_slab as slab\n",
    "\n",
    "d = 0.5                                   # Slab thickness, cm\n",
    "m_max = 10                                # Number of image sources to include\n",
    "rho = 2.5                                 # Lateral source-detector separation, cm\n",
    "db = 1e-8\n",
    "\n",
    "msd = common.msd_brownian(tau, db)\n",
    "g1_norm = slab.g1_reflectance_norm(msd, mua, musp, rho, n, lambda0, d, m_max)\n",
    "\n",
    "g2_norm = 1 + beta * g1_norm**2\n",
    "plot_g1_g2(tau, g1_norm, g2_norm)"
   ],
   "id": "a1ec175f51a5e5eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now try to verify that the slab solution in reflectance is the same as the semi-infinite solution\n",
    "when we only consider the first image source (i.e., set `m_max = 0`)."
   ],
   "id": "49c10e18364f2178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace the zeros initialization with your code...\n",
    "g1_semi_inf = np.zeros_like(tau)\n",
    "g1_slab_m0 = np.zeros_like(tau)\n",
    "\n",
    "# Plotting\n",
    "plt.semilogx(tau, g1_semi_inf, label='Semi-infinite')\n",
    "plt.semilogx(tau, g1_slab_m0, '--', label='Slab m=0')\n",
    "plt.xlabel(r'$\\tau$ (s)')\n",
    "plt.ylabel(r'$g_1$')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f094d7f6f783bf66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, let's study the convergence of the slab solution as we increase the number of image sources `m_max`.\n",
    "Try simulating `g1` and `g2` in the transmittance geometry for different values of `m_max`."
   ],
   "id": "33f8603e233f3b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mua = 0.05\n",
    "musp = 10\n",
    "d = 0.5\n",
    "rho = 2  # Lateral source-detector separation, in transmittance it's usually 0 but here we want to see the effect of rho on convergence\n",
    "db = 1e-8\n",
    "msd = common.msd_brownian(tau, db)\n",
    "m_max_vals = []  # Your code here...\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "g1_all = np.zeros((len(m_max_vals), len(tau)))\n",
    "g2_all = np.zeros((len(m_max_vals), len(tau)))\n",
    "\n",
    "plot_g1_g2(tau, g1_all, g2_all, labels=m_max_vals)"
   ],
   "id": "ec6b0c35276d7f22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The parameters defined above were chosen to highlight the effect of increasing `m_max`. Try changing them\n",
    "to see how they affect the convergence behavior. You should observe that higher absorption coefficients, thicker slabs,\n",
    "and shorter source-detector separations require fewer image sources for convergence."
   ],
   "id": "738174751c0f0d89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Two-layer geometry\n",
    "\n",
    "Finally, let's simulate the autocorrelation for a two-layer geometry.\n",
    "The appropriate `g1_norm` function can be found in the `forward.bilayer` module.\n",
    "\n",
    "The main difference with respect to the previous cases is that now we are dealing with two different media,\n",
    "each with its scatterer motion. As such, in `forward.bilayer.g1_norm`, the `msd` argument is a 2D array of shape\n",
    "`(2, len(tau))`, such that `msd[0, :]` is the mean-square displacement for the upper layer's scatterers and\n",
    "`msd[1, :]` is the one relative to the lower layer. Of course, the scatterer motion models can be different in the two layers.\n",
    "\n",
    "Additionally, the analytical expression of the two-layer geometry contains an integration over all (positive) spatial frequencies,\n",
    "which in the implementation is truncated to the interval `(0, q_max)`, where `q_max` is an input parameter. As such, `q_max` should be\n",
    "chosen sufficiently high to ensure convergence of the solution.\n",
    "\n",
    "Let's see an example. For simplicity, we'll consider Brownian motion in both layers."
   ],
   "id": "3f1b15ba33049ec0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import fit_dcs.forward.bilayer as bilayer\n",
    "\n",
    "db_up = 1e-8\n",
    "db_dn = 6e-8\n",
    "msd_up = common.msd_brownian(tau, db_up)\n",
    "msd_dn = common.msd_brownian(tau, db_dn)\n",
    "msd = np.vstack((msd_up, msd_dn))  # Shape (2, len(tau))\n",
    "mua_up = 0.1\n",
    "mua_dn = 0.2\n",
    "musp_up = 8\n",
    "musp_dn = 10\n",
    "n = 1.4\n",
    "d = 1  # Thickness of the upper layer, cm\n",
    "rho = 3\n",
    "q_max = 80  # 1/cm\n",
    "\n",
    "g1_norm = bilayer.g1_norm(msd, mua_up, mua_dn, musp_up, musp_dn, n, d, rho, lambda0, q_max)\n",
    "g2_norm = 1 + beta * g1_norm**2\n",
    "\n",
    "plot_g1_g2(tau, g1_norm, g2_norm)"
   ],
   "id": "5622ea0aaecbc1c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To study the convergence of the solution, try simulating `g1` and `g2` for different values of `q_max`.",
   "id": "36c53b2b51cb2019"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "q_max_vals = []  # Your code here...\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "g1_all = np.zeros((len(q_max_vals), len(tau)))\n",
    "g2_all = np.zeros((len(q_max_vals), len(tau)))\n",
    "\n",
    "plot_g1_g2(tau, g1_all, g2_all, labels=q_max_vals)"
   ],
   "id": "81f6eb0d5c9b3b03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now try investigating the effect of the lower layer's Brownian diffusion coefficient `db_dn` on the autocorrelation functions.\n",
    "Generate `g1` and `g2` for different values of `db_dn`, while keeping `db_up` fixed.\n",
    "You should use a value of `q_max` that ensures convergence, which you can determine from the previous exercise."
   ],
   "id": "dd8eea70f3ec676d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "db_dn_vals = []  # Your code here...\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "g1_all = np.zeros((len(db_dn_vals), len(tau)))\n",
    "g2_all = np.zeros((len(db_dn_vals), len(tau)))\n",
    "\n",
    "plot_g1_g2(tau, g1_all, g2_all, labels=db_dn_vals)"
   ],
   "id": "efaf80452753ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, repeat the previous exercise with a different source-detector separation `rho`.\n",
    "You should observe that the sensitivity to the lower layer's dynamics increases with `rho`.\n",
    "\n",
    "You are also free to change other parameters (e.g., upper layer thickness `d`, scatterer motion model, ...) to see how they affect the results."
   ],
   "id": "4038cd5ec980605e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding noise\n",
    "\n",
    "So far all the curves we generated were noiseless, as they were obtained using the analytical solutions to the correlation diffusion equation.\n",
    "In this section, we will see how to add realistic noise to the simulated `g2` curves.\n",
    "\n",
    "The `utils.noise` module offers noise-related functionalities, most importantly the expression for the standard deviation of `g2_norm` as a\n",
    "function of time delay `tau`.\n",
    "This model was obtained for an exponentially decaying `g2_norm`, that is, `g2_norm = 1 + beta * exp(-tau/tau_c)`, which works best at short\n",
    "time delays.\n",
    "\n",
    "Let's plot the standard deviation. To get more realistic results, we will load the `tau` which comes from a hardware correlator,\n",
    "rather than creating it ourselves as we did earlier. Additionally, the standard deviation depends on integration time, measurement countrate,\n",
    "decay time constant, and number of detected speckles, so we will need to define those as well."
   ],
   "id": "5ac232c8c610de77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from fit_dcs.utils import noise\n",
    "\n",
    "tau = np.load(\"data/tau.npz\")[\"tau_hardware\"]\n",
    "\n",
    "t_integration = 1   # Integration time, s\n",
    "countrate = 70_000  # Measurement countrate, Hz\n",
    "tau_c = 1e-5        # Exponential decay time constant, s\n",
    "n_speckle = 1       # Number of speckles detected in parallel\n",
    "sigma = noise.sigma_g2_norm(tau, t_integration, countrate, beta, tau_c, n_speckle)\n",
    "\n",
    "# Plotting\n",
    "plt.semilogx(tau, sigma)\n",
    "plt.xlabel(r\"$\\tau$ (s)\")\n",
    "plt.ylabel(r\"$\\sigma$\")\n",
    "plt.title(r\"Standard deviation of $g_2 (\\tau)$\")\n",
    "plt.show()"
   ],
   "id": "f2a17ed5cd73ba86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We notice two features about this plot:\n",
    "1. First, the step-like behavior, which stems from the stepped bin width used in the correlator.\n",
    "2. Second, the non-decreasing trend: while the noise initially decreases with `tau`, it later increases again starting from about 1 ms.\n",
    "This behavior is inconsistent with experimental observations, which show a decreasing trend over the whole time delay range.\n",
    "This failure of the noise model at high delays has been previously observed in the scientific literature.\n",
    "\n",
    "Depending on our application, we might want to modify the output of this function, for example by manually setting `sigma`\n",
    "at the late bins to its minimum value to ensure a decreasing trend:"
   ],
   "id": "af70810c22ddf22e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "idx_last_good = np.argmin(sigma)\n",
    "sigma_corrected = sigma.copy()\n",
    "sigma_corrected[idx_last_good + 1:] = sigma_corrected[idx_last_good]\n",
    "plt.semilogx(tau, sigma, label=\"Original\")\n",
    "plt.semilogx(tau, sigma_corrected, linestyle=\"\", marker=\".\", label=\"Corrected\")\n",
    "plt.xlabel(r\"$\\tau$ (s)\")\n",
    "plt.ylabel(r\"$\\sigma$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "12a4c8b1d27e992f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have the noise model, we can use it to add noise to our autocorrelation curves to mimic a real measurement. To do this,\n",
    "once we have calculated the noiseless `g2_norm`, we sample from a Gaussian distribution with mean `g2_norm` and standard deviation\n",
    "`sigma`.\n",
    "Remember that the `sigma_g2_norm` function takes as input `tau_c`, the time constant of the exponential decay. To estimate this, the\n",
    "noiseless `g2_norm` curve should be fitted with the exponential function defined above.\n",
    "\n",
    "The `noise` module provides a `NoiseAdder` class that automates this process. The constructor take as input the integration time,\n",
    "countrate, beta, and number of speckles, while the `add_noise` method takes `tau` and `g2_norm` as inputs and returns the noisy\n",
    "`g2_norm_noisy`.\n",
    "\n",
    "Let's see an example. For simplicity's sake, we'll use the semi-infinite geometry with Brownian motion again, but the noise addition\n",
    "can be applied to any `g2_norm` curve."
   ],
   "id": "c29679b5ea1dc611"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mua = 0.1\n",
    "musp = 10\n",
    "rho = 2.5\n",
    "n = 1.4\n",
    "db = 1e-8\n",
    "beta = 0.5\n",
    "\n",
    "msd = common.msd_brownian(tau, db)\n",
    "g1_norm = hsi.g1_norm(msd, mua, musp, rho, n, lambda0)\n",
    "g2_norm = 1 + beta * g1_norm**2\n",
    "\n",
    "t_integration = 1   # Integration time, s\n",
    "countrate = 70_000  # Measurement countrate, Hz\n",
    "n_speckle = 1       # Number of speckles detected in parallel\n",
    "\n",
    "noise_adder = noise.NoiseAdder(t_integration, countrate, beta, n_speckle)\n",
    "g2_norm_noisy = noise_adder.add_noise(tau, g2_norm)\n",
    "\n",
    "# Plotting\n",
    "plt.semilogx(tau, g2_norm, label='Noiseless')\n",
    "plt.semilogx(tau, g2_norm_noisy, marker='.', linestyle='', label='Noisy')\n",
    "plt.xlabel(r'$\\tau$ (s)')\n",
    "plt.ylabel(r'$g_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f5a1c6ce5e1b7d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice how the noise decreases with increasing `tau`, as expected. However, for large `tau`, it increases again due to\n",
    "the limitations of the noise model discussed earlier. To mitigate this, the `NoiseAdder` class constructor also accepts\n",
    "an optional boolean argument `ensure_decreasing` (default `False`): when set to `True`, it applies the correction we saw earlier\n",
    "to ensure a decreasing noise trend."
   ],
   "id": "991185cc89ad6eed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noise_adder = noise.NoiseAdder(t_integration, countrate, beta, n_speckle, ensure_decreasing=True)\n",
    "g2_norm_noisy = noise_adder.add_noise(tau, g2_norm)\n",
    "\n",
    "# Plotting\n",
    "plt.semilogx(tau, g2_norm, label='Noiseless')\n",
    "plt.semilogx(tau, g2_norm_noisy, marker='.', linestyle='', label='Noisy')\n",
    "plt.xlabel(r'$\\tau$ (s)')\n",
    "plt.ylabel(r'$g_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "a7197908e374fdb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, now the noise decreases with increasing `tau`, which is more realistic.\n",
    "\n",
    "Now try simulating several noisy autocorrelation curves for different Brownian diffusion coefficients `db`.\n",
    "Start by creating the noiseless curves `g2_all`, then use the `NoiseAdder` class to add noise to each of them.\n",
    "Note that `NoiseAdder` works with 2D arrays as well, so you can pass the entire `g2_all` array at once and get\n",
    "the noisy version `g2_all_noisy` directly back."
   ],
   "id": "efb6eae4dfa97037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "db_vals = []  # Your code here...\n",
    "\n",
    "# Noise parameters, feel free to change them\n",
    "t_integration = 1\n",
    "countrate = 70_000\n",
    "n_speckle = 1\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "g1_all = np.zeros((len(db_vals), len(tau)))\n",
    "g2_all = np.zeros((len(db_vals), len(tau)))\n",
    "g2_all_noisy = np.zeros((len(db_vals), len(tau)))\n",
    "\n",
    "# Plotting\n",
    "for i in range(len(db_vals)):\n",
    "    plt.semilogx(tau, g2_all[i, :], alpha=0.3, color=f\"C{i}\")\n",
    "    plt.semilogx(tau, g2_all_noisy[i, :], '.', color=f\"C{i}\")\n",
    "plt.xlabel(r'$\\tau$ (s)')\n",
    "plt.ylabel(r'$g_2$')\n",
    "plt.show()"
   ],
   "id": "e104f69577656d06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, try changing the noise parameters (integration time, countrate, number of speckles) to see how they affect the noisy curves.",
   "id": "9f6a184cb5deb74d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyzing DCS data\n",
    "\n",
    "In addition to forward modeling, the `Fit-DCS` toolbox also provides tools for solving the inverse problem, that is,\n",
    "extracting scatterer dynamics from measured autocorrelation functions. These are implemented in the `inverse` module, which\n",
    "comprises two submodules: `fit` and `mbl_homogeneous`.\n",
    "\n",
    "We will start with the `fit` submodule, which contains the general-purpose `Fitter` class for fitting DCS autocorrelation data using\n",
    "arbitrary forward models.\n",
    "\n",
    "Let's start by creating some synthetic data to fit. We'll start by simulating `db` changes over time to mimic an occlusion experiment."
   ],
   "id": "2207bda6a75f8d5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "baseline_value = 5e-9\n",
    "baseline_length = 20\n",
    "occlusion_value = 3e-10\n",
    "occlusion_length = 20\n",
    "peak_value = 6 * baseline_value\n",
    "rising_time = 5\n",
    "falling_time = 20\n",
    "total_dur = baseline_length + occlusion_length + rising_time + falling_time\n",
    "db = np.zeros(total_dur)\n",
    "db[:baseline_length] = baseline_value\n",
    "db[baseline_length:baseline_length + occlusion_length] = occlusion_value\n",
    "db[baseline_length + occlusion_length:baseline_length + occlusion_length + rising_time] = np.linspace(occlusion_value, peak_value, rising_time)\n",
    "db[baseline_length + occlusion_length + rising_time:baseline_length + occlusion_length + rising_time + falling_time] = np.linspace(peak_value, baseline_value, falling_time)\n",
    "\n",
    "plt.plot(db)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "plt.show()"
   ],
   "id": "9a4c1343ec801e55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now use the semi-infinite forward model to simulate `g2_norm` curves for each `db` value, and add noise to them using the `NoiseAdder` class.\n",
    "Remember that `g2_norm` should be a 2D array with shape `(total_dur, len(tau))`."
   ],
   "id": "c551bcd475f382a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mua = 0.1\n",
    "musp = 10\n",
    "rho = 2.5\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "g1_all = np.zeros((total_dur, len(tau)))\n",
    "g2_all = np.zeros((total_dur, len(tau)))\n",
    "g2_all_noisy = np.zeros((total_dur, len(tau)))\n",
    "\n",
    "# Plotting a few curves\n",
    "def preview_g2_curves(tau, g2_multi_noiseless, g2_multi_noisy, step=10):\n",
    "    \"\"\"\n",
    "    Helper function to plot noiseless and noisy g2 curves every 'step' iterations.\n",
    "    \"\"\"\n",
    "    n_curves = g2_all.shape[0]\n",
    "    for i in range(0, n_curves, step):\n",
    "        plt.semilogx(tau, g2_multi_noiseless[i, :], color=f\"C{i//step}\", label=i)\n",
    "        plt.semilogx(tau, g2_multi_noisy[i, :], '.', color=f\"C{i//step}\")\n",
    "    plt.xlabel(r'$\\tau$ (s)')\n",
    "    plt.ylabel(r'$g_2$')\n",
    "    plt.legend(title=\"Iteration\")\n",
    "    plt.show()\n",
    "\n",
    "preview_g2_curves(tau, g2_all, g2_all_noisy, step=10)"
   ],
   "id": "1d48e01f969f3ae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Semi-infinite fitting with Brownian motion\n",
    "\n",
    "The `inverse.fit.Fitter` class is used by instantiating it with the appropriate parameters and then calling its `fit()` method\n",
    "to perform the fitting procedure on the data.\n",
    "\n",
    "First of all, we need to choose the geometric model to use for fitting. This is done by passing the appropriate `g1_norm` function\n",
    "from the `forward` module as an argument to the `Fitter` class constructor.\n",
    "In our case, we will use the semi-infinite geometry, so we will pass `forward.homogeneous_semi_inf.g1_norm`.\n"
   ],
   "id": "528778964adcce47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "g1_fitting_function = hsi.g1_norm",
   "id": "8e61e63be6707e72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then, we need to define the scatterer motion model to use for fitting. This includes the model itself (e.g., Brownian, ballistic, hybrid),\n",
    "the initial value of the parameter(s) to fit (e.g., `db` and/or `v_ms`), and their bounds for fitting.\n",
    "To this end, the `inverse.fit` module provides the `MSDModelFit` class, allowing to define all these aspects in a convenient way.\n",
    "\n",
    "Let's define a Brownian motion model for fitting, with an initial guess of `1e-8 cm^2/s` for `db` and lower bound `0` (no upper bound)."
   ],
   "id": "853b102c79b6be45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import fit_dcs.inverse.fit as fit\n",
    "\n",
    "msd_model = fit.MSDModelFit(model_name=\"brownian\", params_init={\"db\": 1e-8}, params_bounds={\"db\": (0, None)})"
   ],
   "id": "df19f43627b44844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then, we need to define how to handle the coherence factor `beta` during fitting. This is done by instantiating the `BetaCalculator` class\n",
    "from the `inverse.fit` module, which allows to choose between three different modes for estimating `beta`:\n",
    "- `fixed`: use a fixed value for `beta` during fitting (passed as an argument to the constructor)\n",
    "- `raw`: estimate `beta` from the raw `g2` data before fitting\n",
    "- `fit`: treat `beta` as an additional fitting parameter\n",
    "\n",
    "In our case, since we know the true value of `beta` used to generate the data, we will start simple and use the `fixed` strategy."
   ],
   "id": "57a60dd4ad266d4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "beta_calculator = fit.BetaCalculator(mode=\"fixed\", beta_fixed=beta)",
   "id": "905d34680ea62737",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To define the fitting range we can pass two more parameters: `tau_lims_fit`, a 2-tuple with the extreme values of `tau` used\n",
    "for fitting (that is, `g2_norm` is fitted between `tau_lims_fit[0]` and `tau_lims_fit[1]`), and `g2_lim_fit`, the lowest value\n",
    "of `g2_norm` up to which the fit is done (that is, the points of `g2_norm` after it has crossed `g2_lim_fit` are not used in the\n",
    "fitting). Both `tau_lims_fit` and `g2_lim_fit` are optional parameters: one can pass both, just one, or neither. If both are\n",
    "passed, the fitting range will satisfy both of them. If neither is passed, the whole `g2_norm` curve will be used for fitting.\n",
    "\n",
    "In our case, we will pass both:"
   ],
   "id": "7505cdc19e94af4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tau_lims_fit = (1e-7, 1e-3)\n",
    "g2_lim_fit = 1.13  # Corresponds to g1 = 0.5 at beta = 0.5"
   ],
   "id": "77ebc638ce162d1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lastly, we need to pass the other parameters required by the forward model, such as `mua`, `musp`, `rho`, etc. Since these\n",
    "depend on the geometry, they are passed as keyword arguments to the `Fitter` class constructor, meaning that their names must be\n",
    "specified and must match the names expected by the `g1_norm` function, but they can be passed in any order.\n",
    "In our case, we have `mua`, `musp`, `rho`, `n`, and `lambda0`.\n",
    "\n",
    "Now we have all the ingredients to instantiate the `Fitter` class."
   ],
   "id": "e50cda6d2c136a7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fitter = fit.Fitter(\n",
    "    g1_fitting_function,\n",
    "    msd_model,\n",
    "    beta_calculator,\n",
    "    tau_lims_fit=tau_lims_fit,\n",
    "    g2_lim_fit=g2_lim_fit,\n",
    "    mua=mua,\n",
    "    musp=musp,\n",
    "    rho=rho,\n",
    "    n=n,\n",
    "    lambda0=lambda0\n",
    ")"
   ],
   "id": "5e356aae9ad3e42f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To perform the fit, we call `fitter.fit()`, passing `tau` and the `g2_norm` to fit. Remember that `g2_norm` must be a 2D array\n",
    "with shape `(n_iterations, len(tau))`.\n",
    "\n",
    "The output of the `fit()` method is a dictionary containing the fitting results, including the fitted parameters for each iteration,\n",
    "the estimated `beta` values, the fitted `g2_norm` curves, and more.\n",
    "\n",
    "Additionally, we can specify a `plot_interval` to plot 1 every `plot_interval` curves to check the fitting.\n",
    "If `plot_interval` is not passed, or if it is `0`, no plotting is done."
   ],
   "id": "c19a8690fe66b530"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fitted_data = fitter.fit(tau, g2_all_noisy, plot_interval=20)",
   "id": "6eea9d7504cf7975",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`fitted_data` is a dictionary containing the fitting results. Its values are 1D arrays with length equal to the number of iterations\n",
    "(i.e., the first dimension of `g2_norm`). Let's check its keys."
   ],
   "id": "9b4bf7ebdf140776"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(fitted_data.keys())",
   "id": "dd758064337ccdf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, `fitted_data` includes the fitted parameters (`db` in our case), the estimated `beta` values, and the `chi2` and `r2` fitting\n",
    "quality metrics.\n",
    "\n",
    "Let's plot the fitted `db` values against the ground truth."
   ],
   "id": "7d98c14e867796fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(db, label=\"Ground truth\")\n",
    "plt.plot(fitted_data[\"db\"], '.', label=\"Fitted\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "8fdb8007e2d7ef49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since we fixed `beta` to its ground truth value, the fit worked quite well.\n",
    "\n",
    "Now try changing the `BetaCalculator` to use the `raw` mode instead of `fixed`, and see how it affects the fitting results.\n",
    "See the documentation of the `BetaCalculator` class for more details on how to use the `raw` mode.\n",
    "Remember to re-instantiate the `Fitter` class with the new `BetaCalculator`."
   ],
   "id": "ea8744b541842293"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace the zeros initialization with your code...\n",
    "fitted_data = {\"db\": np.zeros_like(db), \"beta\": np.zeros_like(db), \"chi2\": np.zeros_like(db), \"r2\": np.zeros_like(db)}\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "axs[0].plot(db, label=\"Ground truth\")\n",
    "axs[0].plot(fitted_data[\"db\"], '.', label=\"Fitted\")\n",
    "axs[0].set_ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.full_like(db, beta), label=\"Ground truth\")\n",
    "axs[1].plot(fitted_data[\"beta\"], '.', label=\"Estimated\")\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(r\"$\\beta$\")\n",
    "axs[1].set_ylim(0.4, 0.6)\n",
    "plt.show()"
   ],
   "id": "ad51aa260e2b2909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since the `raw` mode estimates `beta` from the noisy data before fitting, the results are obviously noisier than in the previous case.\n",
    "You can try changing the portion of the `g2_norm` curve used for estimating `beta` to see if you can improve the results.\n",
    "\n",
    "Finally, try using the `fit` mode for the `BetaCalculator`, which treats `beta` as an additional fitting parameter. Again,\n",
    "see the documentation of the `BetaCalculator` class for more details on how to use the `fit` mode."
   ],
   "id": "9999b1ba095185a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace the zeros initialization with your code...\n",
    "fitted_data = {\"db\": np.zeros_like(db), \"beta\": np.zeros_like(db), \"chi2\": np.zeros_like(db), \"r2\": np.zeros_like(db)}\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "axs[0].plot(db, label=\"Ground truth\")\n",
    "axs[0].plot(fitted_data[\"db\"], '.', label=\"Fitted\")\n",
    "axs[0].set_ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(np.full_like(db, beta), label=\"Ground truth\")\n",
    "axs[1].plot(fitted_data[\"beta\"], '.', label=\"Estimated\")\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(r\"$\\beta$\")\n",
    "axs[1].set_ylim(0.4, 0.6)\n",
    "plt.show()"
   ],
   "id": "ff0b14d7931757d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You should see that in this case fitting `beta` along with `db` yields better results than the `raw` mode.",
   "id": "d3c041182f005d0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison of different geometric models\n",
    "\n",
    "The `Fitter` class is flexible and allows using different geometric models for fitting by simply passing a different\n",
    "`g1_norm` function from the `forward` module and adjusting the keyword arguments passed to the constructor accordingly.\n",
    "\n",
    "Let's try to generate the same synthetic data as before, but this time using the slab geometry in transmittance\n",
    "with thickness `d = 1 cm`."
   ],
   "id": "cc1173e7be24ef45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rho = 0  # Lateral source-detector separation, in transmittance it's usually 0\n",
    "d = 1\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "g1_all = np.zeros((total_dur, len(tau)))\n",
    "g2_all = np.zeros((total_dur, len(tau)))\n",
    "g2_all_noisy = np.zeros((total_dur, len(tau)))\n",
    "\n",
    "preview_g2_curves(tau, g2_all, g2_all_noisy)"
   ],
   "id": "fbec715a64069c1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now try fitting this new dataset using two different geometric models:\n",
    "- the slab geometry in transmittance\n",
    "- the semi-infinite geometry with `rho = d` (to mimic transmittance)\n",
    "\n",
    "In both cases, use the Brownian motion model. You are free to choose the `BetaCalculator` mode and fitting ranges as you prefer."
   ],
   "id": "c59a9641517e8885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace the zeros initialization with your code...\n",
    "fitted_data_slab = {\"db\": np.zeros_like(db), \"beta\": np.zeros_like(db), \"chi2\": np.zeros_like(db), \"r2\": np.zeros_like(db)}\n",
    "fitted_data_semi_inf = {\"db\": np.zeros_like(db), \"beta\": np.zeros_like(db), \"chi2\": np.zeros_like(db), \"r2\": np.zeros_like(db)}\n",
    "\n",
    "# Plotting\n",
    "plt.plot(db, label=\"Ground truth\")\n",
    "plt.plot(fitted_data_slab[\"db\"], '.', label=\"Fitted slab\")\n",
    "plt.plot(fitted_data_semi_inf[\"db\"], '.', label=\"Fitted semi-inf\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "e3b7288a531adcb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You should observe that the semi-infinite model, while capturing the general trend, underestimates the true `db` values compared\n",
    "to the slab model. You can try changing the simulation parameters (e.g., optical properties, slab thickness...) to see how they\n",
    "affect the fitting results, i.e. how much error is introduced by using the semi-infinite approximation."
   ],
   "id": "d64f7d37dcb06069"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-curve fitting\n",
    "\n",
    "Many DCS devices feature multiple detection channels to improve the SNR of the measurement. When all of them are used at the same\n",
    "source-detector separation, they can be averaged to obtain a single `g2_norm` curve with higher SNR.\n",
    "However, when the detection channels are located at different source-detector separations, each of them measures a different\n",
    "`g2_norm` curve, and simply averaging them would lead to incorrect results.\n",
    "\n",
    "As such, the `Fitter` class can also handle multi-curve fitting,\n",
    "that is, fitting multiple `g2_norm` curves simultaneously to extract a single set of scatterer dynamics parameters. Using this feature\n",
    "is as easy as passing multiple `rho` values as a tuple and a 3D array for `g2_norm`, where:\n",
    "- the first dimension corresponds to different iterations\n",
    "- the second dimension corresponds to different source-detector separations\n",
    "- the third dimension corresponds to delay times\n",
    "\n",
    "Note that from an implementation standpoint, there is nothing special about `rho`: any other forward model parameter can be\n",
    "passed as a tuple to indicate that multiple values are to be used, and the `Fitter` class will handle it correctly, though\n",
    "the most common use case is varying `rho`.\n",
    "\n",
    "Let's see an example. We will simulate `g2_norm` curves at two different source-detector separations, `rho = 1.5 cm` and `rho = 3.0 cm`,\n",
    "using the semi-infinite geometry and Brownian motion with the same `db` changes as before.\n",
    "\n",
    "*Side note: `NoiseAdder` expects 2D arrays as input, but we can simply reshape our 3D `g2_all` array to 2D\n",
    "before passing it to the `add_noise()` method, and then reshape the output back to 3D. This is completely safe,\n",
    "since the noise addition is done independently for each curve.*"
   ],
   "id": "4b0d79d08a6e4c19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rho_vals = (1.5, 3.0)\n",
    "\n",
    "# Noiseless g2_all\n",
    "g1_all = np.zeros((total_dur, len(rho_vals), len(tau)))\n",
    "for i_rho in range(len(rho_vals)):\n",
    "    for i_iter in range(total_dur):\n",
    "        msd = common.msd_brownian(tau, db[i_iter])\n",
    "        g1_all[i_iter, i_rho, :] = hsi.g1_norm(msd, mua, musp, rho_vals[i_rho], n, lambda0)\n",
    "g2_all = 1 + beta * g1_all**2\n",
    "\n",
    "# Noisy g2_all_noisy\n",
    "t_integration = 1\n",
    "countrate = 70_000\n",
    "n_speckle = 1\n",
    "noise_adder = noise.NoiseAdder(t_integration, countrate, beta, n_speckle, ensure_decreasing=True)\n",
    "g2_all_noisy = noise_adder.add_noise(tau, g2_all.reshape(-1, len(tau))).reshape(g2_all.shape)  # Reshape to 2D for noise addition and back to 3D\n",
    "\n",
    "# Plotting a few curves\n",
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "for i_rho in range(len(rho_vals)):\n",
    "    ax = axs[i_rho]\n",
    "    ax.set_title(f\"rho = {rho_vals[i_rho]} cm\")\n",
    "    ax.set_ylabel(r\"$g_2$\")\n",
    "    for i in range(0, total_dur, 10):\n",
    "        ax.semilogx(tau, g2_all[i, i_rho, :], color=f\"C{i//10}\", alpha=0.3)\n",
    "        ax.semilogx(tau, g2_all_noisy[i, i_rho, :], '.', color=f\"C{i//10}\")\n",
    "axs[1].set_xlabel(r'$\\tau$ (s)')\n",
    "plt.show()"
   ],
   "id": "b197799b24c0788b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now let's fit this multi-curve dataset using the semi-infinite geometry and Brownian motion. We simply need to pass\n",
    "`rho_vals` as the `rho` argument to the `Fitter` class constructor, and the 3D array `g2_all_noisy` to the `fit()` method."
   ],
   "id": "4e0430fb155eeec0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "beta_calculator = fit.BetaCalculator(mode=\"raw\", tau_lims=(1e-7, 3e-7))\n",
    "fitter = fit.Fitter(\n",
    "    hsi.g1_norm,\n",
    "    msd_model,\n",
    "    beta_calculator,\n",
    "    tau_lims_fit,\n",
    "    g2_lim_fit,\n",
    "    mua=mua,\n",
    "    musp=musp,\n",
    "    rho=rho_vals,\n",
    "    n=n,\n",
    "    lambda0=lambda0\n",
    ")\n",
    "fitted_data = fitter.fit(tau, g2_all_noisy, plot_interval=30)"
   ],
   "id": "14b95f287262dac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If `plot_interval` is specified, the fitting plots will show all the curves for each iteration, automatically creating the necessary\n",
    "subplots. The first subplot corresponds to the first `rho` value, the second subplot to the second `rho` value, and so on.\n",
    "\n",
    "The output `fitted_data` dictionary is mostly the same as in the single-curve fitting case, with the only difference that now\n",
    "multiple `beta` values are estimated (one for each `rho` value), and thus separate keys are created in the dictionary: `beta_0`, `beta_1`, etc."
   ],
   "id": "804dc2c797995e7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(fitted_data.keys())",
   "id": "56c0c4e0d5c724a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now plot the fitted `db` values against the ground truth as before.",
   "id": "61fbd58c04f5cbe5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(db, label=\"Ground truth\")\n",
    "plt.plot(fitted_data[\"db\"], '.', label=\"Fitted\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "ac3036416f2c02b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now try it yourself! Create a multi-curve dataset using different `rho` values, and fit it using the `Fitter` class.\n",
    "You are free to choose the simulation and fitting parameters, such as the geometric model, number of `rho` values, etc."
   ],
   "id": "910cd072cd096044"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rho_vals = ()  # Your code here...\n",
    "\n",
    "# Replace the zeros initialization with your code...\n",
    "fitted_data = {\"db\": np.zeros_like(db), \"chi2\": np.zeros_like(db), \"r2\": np.zeros_like(db)}\n",
    "\n",
    "# Plotting\n",
    "plt.plot(db, label=\"Ground truth\")\n",
    "plt.plot(fitted_data[\"db\"], '.', label=\"Fitted\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "9a860093837a7beb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Two-layer model\n",
    "\n",
    "Multi-curve fitting is especially useful when dealing with layered geometries, where the sensitivity to different layers\n",
    "varies with source-detector separation. The `Fitter` class works seamlessly with geometries that feature multiple scatterer\n",
    "motion models, such as the two-layer geometry. We just need to define two `MSDModelFit` instances, one for each layer, and pass them\n",
    "as a tuple to the `Fitter` class constructor.\n",
    "\n",
    "Let's see an example. We will simulate `g2_norm` curves at two different source-detector separations, `rho = (1, 3) cm`,\n",
    "using the bilayer geometry with Brownian motion in both layers. The upper layer will have thickness `d = 0.5 cm` and constant\n",
    "`db_up = 1e-8 cm^2/s`, while the lower layer will have `db_dn` changing over time as before."
   ],
   "id": "7673fbd9963e66ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "db_up = 1e-8\n",
    "d = 0.5\n",
    "rho_vals = (1, 3)\n",
    "mua_up = 0.1\n",
    "mua_dn = 0.2\n",
    "musp_up = 8\n",
    "musp_dn = 10\n",
    "q_max = 60\n",
    "\n",
    "msd = np.zeros((2, len(tau)))\n",
    "msd[0, :] = common.msd_brownian(tau, db_up)  # Upper layer MSD\n",
    "for i_rho in range(len(rho_vals)):\n",
    "    for i_iter in range(total_dur):\n",
    "        msd[1, :] = common.msd_brownian(tau, db[i_iter])  # Lower layer MSD\n",
    "        g1_all[i_iter, i_rho, :] = bilayer.g1_norm(msd, mua_up, mua_dn, musp_up, musp_dn, n, d, rho_vals[i_rho], lambda0, q_max)\n",
    "g2_all = 1 + beta * g1_all**2\n",
    "\n",
    "noise_adder = noise.NoiseAdder(t_integration, countrate, beta, n_speckle, ensure_decreasing=True)\n",
    "g2_all_noisy = noise_adder.add_noise(tau, g2_all.reshape(-1, len(tau))).reshape(g2_all.shape)\n",
    "\n",
    "# Plotting a few curves\n",
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "for i_rho in range(len(rho_vals)):\n",
    "    ax = axs[i_rho]\n",
    "    ax.set_title(f\"rho = {rho_vals[i_rho]} cm\")\n",
    "    ax.set_ylabel(r\"$g_2$\")\n",
    "    for i in range(0, total_dur, 10):\n",
    "        ax.semilogx(tau, g2_all[i, i_rho, :], color=f\"C{i//10}\", alpha=0.3)\n",
    "        ax.semilogx(tau, g2_all_noisy[i, i_rho, :], '.', color=f\"C{i//10}\")\n",
    "axs[1].set_xlabel(r'$\\tau$ (s)')\n",
    "plt.show()"
   ],
   "id": "d0e07669189fe2ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now let's fit this multi-curve dataset using the bilayer geometry with Brownian motion in both layers.\n",
    "We need to define two `MSDModelFit` instances, one for each layer, and pass them as a tuple to the `Fitter` class constructor.\n",
    "To only fit `db_dn`, we will fix `db_up` to its ground truth value by setting both its initial value and bounds accordingly.\n",
    "\n",
    "Additionally, since the bilayer model is more computationally intensive, we will speed up the fitting process by using multiple workers.\n",
    "To do so, we pass the `num_workers` argument to the `fit()` method, specifying the number of parallel workers to use."
   ],
   "id": "89eca0ae276490ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "msd_model_up = fit.MSDModelFit(model_name=\"brownian\", params_init={\"db\": 1e-8}, params_bounds={\"db\": (1e-8, 1e-8)})\n",
    "msd_model_dn = fit.MSDModelFit(model_name=\"brownian\", params_init={\"db\": 1e-8}, params_bounds={\"db\": (0, None)})\n",
    "msd_models = (msd_model_up, msd_model_dn)\n",
    "\n",
    "beta_calculator = fit.BetaCalculator(mode=\"fixed\", beta_fixed=beta)\n",
    "fitter = fit.Fitter(\n",
    "    bilayer.g1_norm,\n",
    "    msd_models,\n",
    "    beta_calculator,\n",
    "    tau_lims_fit,\n",
    "    g2_lim_fit,\n",
    "    mua_up=mua_up,\n",
    "    mua_dn=mua_dn,\n",
    "    musp_up=musp_up,\n",
    "    musp_dn=musp_dn,\n",
    "    d=d,\n",
    "    n=n,\n",
    "    rho=rho_vals,\n",
    "    lambda0=lambda0,\n",
    "    q_max=q_max\n",
    ")\n",
    "fitted_data = fitter.fit(tau, g2_all_noisy, num_workers=15)\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "axs[0].plot(db, label=\"Ground truth\")\n",
    "axs[0].plot(fitted_data[\"db_1\"], '.', label=\"Fitted\")\n",
    "axs[0].set_ylabel(r\"$D_B$ lower layer (cm$^2$/s)\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(np.full_like(db, db_up), label=\"Ground truth\")\n",
    "axs[1].plot(fitted_data[\"db_0\"], '.', label=\"Fitted\")\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(r\"$D_B$ upper layer (cm$^2$/s)\")\n",
    "axs[1].legend()\n",
    "plt.show()"
   ],
   "id": "72b8c1850e0ebb40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this case, since we fixed both `db_up` and `beta`, the fitting works quite well.\n",
    "\n",
    "Now try changing some of the parameters to see how they affect the fitting results, such as:\n",
    "- letting `db_up` vary during fitting\n",
    "- changing the `BetaCalculator` mode\n",
    "- changing the `rho` values used\n",
    "- changing the upper layer thickness `d`\n",
    "- ..."
   ],
   "id": "e96bf341dc6dd7b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modified Beer-Lambert law\n",
    "\n",
    "Data analysis via the Modified Beer-Lambert law is also supported. The necessary functionalities are implemented in the\n",
    "`inverse.mbl_homogeneous` module. Similarly to `inverse.fit`, it provides a `MBLHomogeneous` class to handle the\n",
    "analysis of the measurement.\n",
    "\n",
    "Remember that the Modified Beer-Lambert law relates *changes* in autocorrelation to *changes* in `db` (or `v_ms`). That is to say,\n",
    "to get absolute values of the parameters of interest, we first need to define a baseline from which changes occur. In order to do so,\n",
    "we need `g2_norm_0`, the autocorrelation in baseline conditions, as well the baseline values of all the parameters: `mua0`, `musp0`,\n",
    "and even `db0`.\n",
    "\n",
    "Currently, only homogeneous geometries and scatterer motion models with a single parameter (i.e., Brownian or ballistic) are supported.\n",
    "\n",
    "Let's see an example. We will create a synthetic dataset similar to the previous ones, with the semi-infinite geometry and Brownian motion."
   ],
   "id": "c240682f21887b12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rho = 2.5\n",
    "g1_all = np.zeros((total_dur, len(tau)))\n",
    "for i in range(total_dur):\n",
    "    msd = common.msd_brownian(tau, db[i])\n",
    "    g1_all[i, :] = hsi.g1_norm(msd, mua, musp, rho, n, lambda0)\n",
    "g2_all = 1 + beta * g1_all**2\n",
    "\n",
    "t_integration = 1\n",
    "countrate = 70_000\n",
    "n_speckle = 1\n",
    "noise_adder = noise.NoiseAdder(t_integration, countrate, beta, n_speckle, ensure_decreasing=True)\n",
    "g2_all_noisy = noise_adder.add_noise(tau, g2_all)\n",
    "\n",
    "# Plotting a few curves\n",
    "for i in range(0, total_dur, 10):\n",
    "    plt.semilogx(tau, g2_all[i, :], color=f\"C{i//10}\", label=i)\n",
    "    plt.semilogx(tau, g2_all_noisy[i, :], '.', color=f\"C{i//10}\")\n",
    "plt.xlabel(r'$\\tau$ (s)')\n",
    "plt.ylabel(r'$g_2$')\n",
    "plt.legend(title=\"Iteration\")\n",
    "plt.show()"
   ],
   "id": "5cfd3adfc2139d39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we need `g2_norm_0`, the baseline autocorrelation curve. While we could simply take the first curve of the noiseless dataset,\n",
    "to be more realistic we will average the first `baseline_length` curves of the noisy dataset to obtain a better estimate of `g2_norm_0`."
   ],
   "id": "13b561f97e372795"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "g2_norm_0 = np.mean(g2_all_noisy[:baseline_length, :], axis=0)\n",
    "plt.semilogx(tau, g2_norm_0)\n",
    "plt.xlabel(r'$\\tau$ (s)')\n",
    "plt.ylabel(r'$g_2$ baseline')\n",
    "plt.show()"
   ],
   "id": "cb79bc12f09c76cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In a real setting `mua0` and `musp0` would come from a TD NIRS measurement, and we could average them over the baseline as we did for `g2_norm_0`.\n",
    "In our case we modeled constant optical properties, so `mua0 = mua` and `musp0 = musp`.\n",
    "\n",
    "As for `db0`, for simplicity we will set it to the ground truth baseline value used to generate the data, that is, `db0 = baseline_value`.\n",
    "In a real setting, `db0` could be estimated via fitting of the baseline `g2_norm_0` curve using the `Fitter` class we saw earlier\n",
    "(which you will be doing in the next exercise)."
   ],
   "id": "994879cbcb6e49b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "db0 = baseline_value",
   "id": "8416ebe61a9d98e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we need a model for scatterer motion, and we can specify it as an instance of the `inverse.mbl_homogeneous.MSDModelMBL` class,\n",
    "similarly to what we did earlier with `inverse.fit.MSDModelFit`. Along with the model, we need to specify the baseline value\n",
    "of the parameter (`db0` in our case)."
   ],
   "id": "7afa5faf285318ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import fit_dcs.inverse.mbl_homogeneous as mbl\n",
    "\n",
    "msd_model = mbl.MSDModelMBL(model_name=\"brownian\", param0=db0)"
   ],
   "id": "e086c1c0f48962ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Lastly, we need to specify the geometric model. Similarly to `Fitter`, we do so by passing the appropriate function to the\n",
    "`MBLHomogeneous` class constructor. In this case, however, the function to be passed is not the fitting `g1_norm`, but rather a function\n",
    "that returns the sensitivity coefficients for the geometry of interest, which can be found in the `forward` module\n",
    "(see their documentation for details). As a side note, it is also possible to pass a custom function for computing sensitivity coefficients.\n",
    "This is useful when calculating them through other means, e.g., Monte Carlo simulations.\n",
    "\n",
    "In our case, we are going to use `forward.homogeneous_semi_inf.d_factors`, which computes the sensitivity coefficients for the\n",
    "homogeneous semi-infinite geometry."
   ],
   "id": "1583b7653d7415ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "d_factors_fn = hsi.d_factors",
   "id": "2bbf12a32120d17b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function, much like `g1_norm`, requires several parameters to compute the sensitivity coefficients, such as `mua`, `musp`, `rho`, etc.\n",
    "As was the case for `Fitter`, these are passed as keyword arguments to the `MBLHomogeneous` class constructor.\n",
    "\n",
    "We are now ready to analyze the data using the Modified Beer-Lambert law. Let's create our `mbl_analyzer` with all the options we defined.\n",
    "Note the difference between `mua` and `mua0` (similarly for `musp` and `musp0`): the former are the current optical properties\n",
    "used for analysis, while the latter are the baseline optical properties used for computing sensitivity coefficients.\n",
    "In this case they are the same, but in a real setting they could differ to account for changes in optical properties during the measurement.\n",
    "Then, `mua` and `musp` would be arrays with length equal to the number of iterations, while `mua0` and `musp0` would be scalars.\n",
    "Note that passing arrays for `mua` and `musp` is supported also by the `Fitter` class, though for simplicity we didn't use this\n",
    "feature in the previous examples."
   ],
   "id": "7a32c1cb0ee603c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mbl_analyzer = mbl.MBLHomogeneous(\n",
    "    g2_norm_0,\n",
    "    d_factors_fn,\n",
    "    msd_model,\n",
    "    mua,\n",
    "    musp,\n",
    "    mua0=mua,\n",
    "    musp0=musp,\n",
    "    rho=rho,\n",
    "    n=n,\n",
    "    lambda0=lambda0\n",
    ")"
   ],
   "id": "83413dc2fdbefbf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can call `mbl_analyzer.fit(tau, g2_all_noisy)` to perform the analysis. Note that plotting during the analysis is not supported, since there is\n",
    "no fitting function to plot. Also, multi-threading is not supported, since the analysis is already very fast.\n",
    "\n",
    "We will typically get warnings saying that we tried to divide by 0, or that we tried to take the logarithm of a negative number. This\n",
    "happens because of the noise, and is completely normal: `g2_norm` at later delay times has a low SNR and results in invalid values of `db`."
   ],
   "id": "f79f1324bc047059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "db_mbl = mbl_analyzer.fit(tau, g2_all_noisy)",
   "id": "b3d9572fc3ecbc49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `fit()` method returns a matrix the same shape as `g2_norm`, containing the value of the parameter of interest (`db`, in our case)\n",
    "for each iteration and lag time. We can plot the mean and standard deviation of `db_mbl` during the baseline to assess which lag\n",
    "times to consider:"
   ],
   "id": "f540ff1425e0e8c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "db_mbl_baseline_avg = np.mean(db_mbl[:baseline_length, :], axis=0)\n",
    "db_mbl_baseline_sd = np.std(db_mbl[:baseline_length, :], axis=0)\n",
    "plt.hlines(baseline_value, tau[0], 4e-4, linestyle=\"--\", color=\"tab:blue\", label=\"Ground truth\")\n",
    "plt.errorbar(tau, db_mbl_baseline_avg, db_mbl_baseline_sd, marker=\"o\", color=\"tab:orange\", label=\"MBL baseline\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"$\\tau$\")\n",
    "plt.ylabel(r\"D$_B$ (cm$^2$/s)\")\n",
    "plt.legend()\n",
    "plt.ylim((0, 2 * baseline_value))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "88ab48c74b61a566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The signal is very noisy at small lag times, but for big values of `tau` the Modified Beer-Lambert law performs poorly\n",
    "(notice the missing points: our `tau` actually extends to `1 s`!). A good trade-off in this case could be `1e-5 s < tau < 3e-5 s`."
   ],
   "id": "6694aba5f8d63c5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask = np.logical_and(tau > 1e-5, tau < 3e-5)\n",
    "db_mbl_signal = np.mean(db_mbl[:, mask], axis=1)\n",
    "plt.plot(db, linestyle=\"--\", label=\"Ground truth\")\n",
    "plt.plot(db_mbl_signal, label=\"MBL\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"D$_B$ (cm$^2$/s)\")\n",
    "plt.show()"
   ],
   "id": "96227a658207b125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now try changing the `tau` interval used for analysis to see how it affects the results. You should observe that\n",
    "earlier delay times give noisier signals, while later delays result in cleaner signals, but tend to underestimate big changes such\n",
    "as those occurring in the hyperemic peak."
   ],
   "id": "5f2d42947bc8c162"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define 3 masks for different tau intervals\n",
    "# Replace the following line with your code...\n",
    "mask1, mask2, mask3 = mask, mask, mask\n",
    "\n",
    "# Apply masks\n",
    "db1 = np.mean(db_mbl[:, mask1], axis=1)\n",
    "db2 = np.mean(db_mbl[:, mask2], axis=1)\n",
    "db3 = np.mean(db_mbl[:, mask3], axis=1)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(db, linestyle=\"--\", label=\"Ground truth\")\n",
    "plt.plot(db1, label=\"Tau interval 1\")\n",
    "plt.plot(db2, label=\"Tau interval 2\")\n",
    "plt.plot(db3, label=\"Tau interval 3\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"D$_B$ (cm$^2$/s)\")\n",
    "plt.show()"
   ],
   "id": "6076d055e765f152",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now try comparing the results of the Modified Beer-Lambert law with those obtained via fitting using the `Fitter` class.\n",
    "Generate a new synthetic dataset using the slab geometry in transmittance, and analyze it using both methods.\n",
    "You are free to choose the fitting parameters and `tau` intervals as you prefer. For the MBL analysis, use the fitting results\n",
    "as baseline values for `db0`."
   ],
   "id": "47d0cb203069390"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Simulation parameters\n",
    "# Your code here...\n",
    "\n",
    "# Noiseless dataset\n",
    "# Your code here...\n",
    "\n",
    "# Noisy dataset\n",
    "# Your code here...\n",
    "\n",
    "# Fitting with Fitter\n",
    "# Replace the zeros initialization with your code...\n",
    "fitted_data = dict(\n",
    "    db=np.zeros_like(db),\n",
    "    beta=np.zeros_like(db),\n",
    "    chi2=np.zeros_like(db),\n",
    "    r2=np.zeros_like(db)\n",
    ")\n",
    "db_fit = fitted_data[\"db\"]\n",
    "\n",
    "# Analysis with MBL\n",
    "# Replace the zeros initialization with your code...\n",
    "db_mbl = np.zeros_like(db)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.plot(db, linestyle=\"--\", label=\"Ground truth\")\n",
    "plt.plot(db_fit, label=\"Fitted\")\n",
    "plt.plot(db_mbl, label=\"MBL\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"D$_B$ (cm$^2$/s)\")\n",
    "plt.show()"
   ],
   "id": "f050ee9c93ac35c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Saving and loading analysis results\n",
    "\n",
    "While there is no built-in functionality to save the analysis results to disk, it is straightforward to do so using\n",
    "standard Python libraries such as `numpy` or `pandas`.\n",
    "\n",
    "`numpy` provides the `np.savez()` function to save multiple arrays to a single binary file in `.npz` format, and the `np.load()`\n",
    "function to load them back. Here's an example of how to save the fitting results obtained with the `Fitter` class."
   ],
   "id": "2a1eafe2ac0f70fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_filename = \"fitting_results.npz\"\n",
    "np.savez(output_filename, db=fitted_data[\"db\"], beta=fitted_data[\"beta\"])"
   ],
   "id": "285cf80681b1c2cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And to load them back:",
   "id": "2d37a997992a1201"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaded_data = np.load(output_filename)\n",
    "db_loaded, beta_loaded, = loaded_data[\"db\"], loaded_data[\"beta\"]\n",
    "plt.plot(db_loaded, label=\"Loaded db\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "2420a2d554e270e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Alternatively, `pandas` provides the `DataFrame.to_csv()` method to save tabular data to a CSV file, and the `pd.read_csv()`\n",
    "function to load it back. This is especially useful if we want to be able to easily inspect and manipulate the results\n",
    "using spreadsheet software.\n",
    "\n",
    "Here's an example of how to save the fitting results using `pandas`. First, we convert the `fitted_data` dictionary to a `DataFrame`,\n",
    "then we save it to a CSV file."
   ],
   "id": "2a22a9ca1607ff81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(fitted_data)\n",
    "results_df.to_csv(\"fitting_results.csv\", index=False)"
   ],
   "id": "8b67f305dd718732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To load the results back:",
   "id": "309dca67f16b2c93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaded_results_df = pd.read_csv(\"fitting_results.csv\")\n",
    "loaded_results_df"
   ],
   "id": "1d1a258ee98d5e5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading real measurement data\n",
    "\n",
    "`Fit-DCS` also provides tools to load real measurement data from different DCS devices. The relevant classes are implemented\n",
    "in the `utils.data_loaders` module, which can read data files from two types of devices: the ALV7004/USB-FAST hardware correlator,\n",
    "and the Swabian TimeTagger20.\n",
    "\n",
    "## Hardware correlator\n",
    "\n",
    "Let's start with the hardware correlator. The `ALVDataLoader` class can read `.asc` files generated by the ALV software (each file\n",
    "corresponds to an iteration). To use it, first we create an instance by passing the list of data files to the class constructor,\n",
    "and then we call the `load_data()` method on the instance.\n",
    "Note that `load_data()` does not return anything, rather, it stores the info about the measurement in the class attributes.\n",
    "\n",
    "We can then call `loader.get_data()` to retrieve the data as a dictionary (see below for details on its contents).\n",
    "\n",
    "Let's see an example. We will load the measurement contained in the `data/PRO_025_T2_Occ` folder, which contains a DCS measurement\n",
    "of a *vastus lateralis* muscle during a cuff occlusion protocol. The `os.listdir()` function is used to get the list of data files\n",
    "in the folder."
   ],
   "id": "31e3cf48d07eb754"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import fit_dcs.utils.data_loaders as data_loaders\n",
    "\n",
    "data_files = os.listdir(\"data/hardware_corr/\")[:-1]   # We exclude the last file, since it's incomplete and can't be read properly\n",
    "data_files = [os.path.join(\"data/hardware_corr/\", file) for file in data_files]\n",
    "loader = data_loaders.DataLoaderALV(data_files)\n",
    "loader.load_data()\n",
    "alv_data = loader.get_data()\n",
    "print(alv_data.keys())"
   ],
   "id": "ab4d7b7cd85d73f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, `data` is a dictionary containing the measurement data, including:\n",
    "- `tau`: 1D array of time delays\n",
    "- `g2_norm`: 3D array of normalized g2 data, with shape `(n_iterations, n_channels, n_bins)`\n",
    "- `countrate`: 2D array of countrate values, with shape `(n_iterations, n_channels)`\n",
    "- `integration_time`: scalar value storing the integration time used for the measurement, in seconds\n",
    "\n",
    "`g2_norm` is the shape requested by the `Fitter` class for multi-curve fitting, streamlining the analysis process.\n",
    "Of course, we can also average over the channels to obtain a 2D array if we want to perform single-curve fitting or a\n",
    "Modified Beer-Lambert law analysis. To this end, `utils.data_loaders` also provides a helper function, `weigh_g2()`,\n",
    "which performs a weighted average of `g2_norm` using the countrate values as weights.\n",
    "\n",
    "Here's how to use it:"
   ],
   "id": "276b705eda60b51d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "g2_avg = data_loaders.weigh_g2(alv_data[\"g2_norm\"], alv_data[\"countrate\"])\n",
    "\n",
    "# Plotting the curves\n",
    "plt.semilogx(alv_data[\"tau\"], g2_avg.T)\n",
    "plt.xlabel(r'$\\tau$ (s)')\n",
    "plt.ylabel(r'$g_2$ averaged')\n",
    "plt.show()"
   ],
   "id": "8e6704ff56909ba2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see, the measured `g2_norm` curves exhibit the afterpulse effect at short delay times, which should be excluded\n",
    "from the analysis. We can either apply a mask to the curves before fitting, or use the `tau_lims_fit` argument of the `Fitter` class\n",
    "to exclude the first delay times during fitting.\n",
    "\n",
    "Now try fitting this dataset using the `Fitter` class. The measurement was taken at `rho = 2.5 cm`, and we can assume\n",
    "typical optical properties for muscle tissue at NIR wavelengths: `mua = 0.2 cm^-1`, `musp = 10 cm^-1`. You are free to choose\n",
    "the scatterer motion model, `BetaCalculator` mode, and fitting parameters as you prefer."
   ],
   "id": "d269ab6aacdbb735"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace the zeros initialization with your code...\n",
    "db_fitted = np.zeros(alv_data[\"g2_norm\"].shape[0])\n",
    "\n",
    "# Plotting\n",
    "plt.plot(db_fitted)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "plt.show()"
   ],
   "id": "723d15c0cc68b22b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TimeTagger\n",
    "\n",
    "Now let's see how to load data from the Swabian TimeTagger20.\n",
    "The `.ttbin` files saved by the TimeTagger require the Swabian libraries to be read, which can be freely downloaded from\n",
    "https://www.swabianinstruments.com/time-tagger/downloads/. Once those are installed (only the Python files are needed), the data loader\n",
    "will function properly.\n",
    "\n",
    "The `.ttbin` files store the raw time tags, from which we compute the autocorrelations. The first step is to define the architecture of\n",
    "the software correlator. Since we are using a multi-tau correlator, the architecture can be specified through the following 3 integers:\n",
    "* `s`: the number of cascaded linear correlators.\n",
    "* `m`: the binning ratio between two consecutive linear correlators (in hardware correlators usually `m = 2`).\n",
    "* `p`: the number of bins in each linear correlator, it must be an integer multiple of `m`\n",
    "\n",
    "While one can manually set `s`, `m`, and `p`, the `utils.timetagger` module provides the `get_correlator_architecture` function, which takes\n",
    "as input the following arguments:\n",
    "* `alpha = p / m`, which should be high enough to guarantee a good signal-to-noise ratio. For example, `alpha = 7` guarantees an\n",
    "error the order of `1e-3`.\n",
    "* `m`.\n",
    "* `tau_max`, the maximum desired value of `tau` up to which to calculate the autocorrelation.\n",
    "* `t0`, the resolution (in s) of the TimeTagger. The Swabian TimeTagger20 has a resolution of 1 ps, so we should set `t0 = 1e-12`.\n",
    "\n",
    "`get_correlator_architecture` will then output `p` and `s`, like so:"
   ],
   "id": "4ab6de4188ec8c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from fit_dcs.utils.timetagger import get_correlator_architecture\n",
    "\n",
    "m = 2\n",
    "(p, s) = get_correlator_architecture(alpha=7, m=m, tau_max=1e-2, t0=1e-12)\n",
    "print(f\"m={m}, p={p}, s={s}\")"
   ],
   "id": "4e06649bb8b8345a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that we have defined the correlator architecture, we can load the data using the `data_loaders.DataLoaderTimeTagger` class, which\n",
    "reads a measurement file and calculates the autocorrelation according to the specified architecture and integration time.\n",
    "\n",
    "Let's load the measurement `2024-05-07_Pulsatility_forearm.ttbin`, contained in `data/software_corr`. Measurement files generated by the\n",
    "TimeTagger consist of a header file (in our case `2024-05-07_Pulsatility_forearm.ttbin`), with information on the measurement, and one or more\n",
    "files which actually store the time tags (in our case there are `3`). We only pass the header file to the data loader, which automatically\n",
    "takes care of loading the time tags from the other files.\n",
    "\n",
    "This measurement features a high countrate (> 1 MHz) on channel 1, and only lasts a few seconds, so we are going to use a short integration\n",
    "time, `30 ms`, to calculate the autocorrelations. To speed up the computation, we can pass the optional argument `tau_start`, which is the\n",
    "minimum value of `tau` from which the autocorrelation is calculated. Since the detectors we use have a `20 ns` dead time, the autocorrelation\n",
    "for `tau < 20 ns` will always be `0`, so we should set `tau_start` to at least `20e-9`. Since early lag times feature the afterpulse peak,\n",
    "we will set it a little higher, to `100 ns`.\n",
    "\n",
    "Note that the `channels` argument must be a list even when using only 1 channel."
   ],
   "id": "78f7f98b3161f132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file = \"data/software_corr/2024-05-07_Pulsatility_forearm.ttbin\"\n",
    "integration_time = 30e-3\n",
    "channels = [1]\n",
    "tau_start = 1e-7\n",
    "loader = data_loaders.DataLoaderTimeTagger(\n",
    "    file,\n",
    "    integration_time=integration_time,\n",
    "    channels=channels,\n",
    "    p=p,\n",
    "    m=m,\n",
    "    s=s,\n",
    "    tau_start=tau_start\n",
    ")"
   ],
   "id": "f2479fffe0a9ab45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Similar to `DataLoaderALV`, we call `loader.load_data()`, which stores the results in the attributes `tau`, `g2_norm`, and `countrate`.\n",
    "`load_data()` calculates the autocorrelations on each channel in parallel, and we can set the maximum number of parallel workers\n",
    "via the `max_workers` argument. If we don't set it, it will default to using all available CPU cores.\n",
    "\n",
    "In this case we are only analyzing 1 channel, so there is no need to use more than 1 worker. Note that if we set a\n",
    "higher number of workers than channels, `load_data()` will simply use the number of channels as `max_workers`."
   ],
   "id": "19b550449629fcac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "loader.load_data(max_workers=1)",
   "id": "a21d8ba06d702922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Just like with `DataLoaderALV`, we can retrieve the data via `loader.get_data()` to get a dictionary with keys `tau`, `g2_norm`,\n",
    "`countrate` and `integration_time`.\n",
    "Additionally, `DataLoaderTimeTagger.get_data()` also accepts a boolean `full` argument (default `True`), which indicates whether to\n",
    "return `tau` and `g2_norm` including all lag times (if `full=True`), or only those for `tau >= tau_start` (if `full=False`).\n",
    "In this case, since we set `tau_start` to `100 ns`, we will set `full=False` to retrieve only the relevant part of the autocorrelation."
   ],
   "id": "af1d87d43e509be5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pulsatility_data = loader.get_data(full=False)\n",
    "tau, g2_norm, countrate = pulsatility_data[\"tau\"], pulsatility_data[\"g2_norm\"], pulsatility_data[\"countrate\"]\n",
    "print(g2_norm.shape)"
   ],
   "id": "5cd55d2ae02659f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that, even though we only analyzed 1 channel, `g2_norm` is still a 3D array with shape `(n_iterations, n_channels, n_bins)`.\n",
    "This is not a problem for the `Fitter` class, which can handle both single-curve and multi-curve fitting seamlessly. However, depending\n",
    "on your downstream analysis, you might want to squeeze the array to remove the singleton dimension corresponding to channels.\n",
    "\n",
    "Now try fitting this dataset using the `Fitter` class. The measurement was taken at `rho = 1 cm`, and we can assume\n",
    "typical optical properties for forearm tissue at NIR wavelengths: `mua = 0.2 cm^-1`, `musp = 10 cm^-1`. You are free to choose\n",
    "the scatterer motion model, `BetaCalculator` mode, and fitting parameters as you prefer."
   ],
   "id": "202e4c19664db80d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace the zeros initialization with your code...\n",
    "db_fitted = np.zeros(g2_norm.shape[0])\n",
    "\n",
    "# Plotting\n",
    "plt.plot(db_fitted)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$D_B$ (cm$^2$/s)\")\n",
    "plt.show()"
   ],
   "id": "bdc41b08ba7c3f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
